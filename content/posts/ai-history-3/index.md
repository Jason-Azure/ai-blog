---
title: "AI的70年（下）：争鸣——机器究竟有没有在思考？"
date: 2026-02-24
draft: false
summary: "AI会下棋、会写诗、会通过律师考试。但它真的在'思考'吗？从Chomsky的语言天赋论到LeCun的世界模型缺失，从随机鹦鹉到具身认知——这是一场关于智能本质的终极辩论。而70年AI之路的最深启示，也许不在技术之中。"
categories: ["AI 基础"]
tags: ["AI历史", "AI争鸣", "世界模型", "具身认知", "RLHF", "对齐"]
weight: 13
ShowToc: true
TocOpen: true
---

> 本文是"AI的70年"系列的最后一篇。建议先阅读 [上篇（梦想与寒冬）](/ai-blog/posts/ai-history-1/) 和 [中篇（复兴与爆发）](/ai-blog/posts/ai-history-2/)

---

## 引言：一个从未解决的老问题

1950年，图灵在论文的第一句话就问了：

> *"Can machines think?"*（机器能思考吗？）

75年后，我们造出了能通过律师资格考试、能写十四行诗、能解微积分题的AI系统。

但这个问题依然没有答案。

事实上，它变得更加尖锐了。因为现在，争论的双方都拿得出证据——而且双方的证据都很有说服力。

这不是一场"聪明人对笨人"的辩论。站在反对方的是图灵奖得主、是当代最伟大的语言学家、是深度学习三巨头之一。站在支持方的也是图灵奖得主、也是深度学习的奠基人。

**他们彼此了解对方的论点，仍然无法说服对方。**

这一章，我们尽量公平地呈现双方的观点。你会发现，这场争论的深度远超"AI行不行"——它触及了人类如何定义"理解"和"思考"这些最基本的概念。

---

## 第十一章：反对派的深层批判——AI到底缺了什么？

### 一、"随机鹦鹉"——Bender & Gebru（2021）

2021年，华盛顿大学语言学教授 **Emily Bender** 和AI伦理研究者 **Timnit Gebru** 发表了一篇论文，标题极具挑衅性：

> ***On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?***
> 《论随机鹦鹉的危险：语言模型是否可以太大了？》

她们的核心论点：

**大语言模型只是在做"统计拼贴"——把训练数据中出现过的语言模式重新排列组合，但对这些词语的含义一无所知。就像一只鹦鹉可以完美模仿人类的对话，但它不理解自己在说什么。**

具体来说：

- LLM的"知识"来源于训练数据的统计分布，不是来源于对世界的理解
- 它生成的"流畅文本"，是高级的模式匹配，不是思考的结果
- 当训练数据中存在偏见，模型会毫不犹豫地复制这些偏见——因为它根本不"知道"什么是对什么是错

这篇论文引发了巨大的风波——Gebru在发表前被Google解雇（她当时是Google AI伦理团队的联合负责人），引发了整个AI行业对企业伦理的激烈讨论。

但抛开政治争议，"随机鹦鹉"这个比喻确实触及了一个核心问题：**流畅地说话，和真正理解自己在说什么，是同一件事吗？**

> **论文卡片**
> Bender et al. (2021), *On the Dangers of Stochastic Parrots*, FAccT 2021
> **一句话意义：** 提出"随机鹦鹉"批判——LLM的流畅输出可能只是高级的统计拼贴，不代表理解。

### 二、世界模型的缺失——Yann LeCun

**Yann LeCun**——深度学习三巨头之一、图灵奖得主、Meta首席AI科学家——是最有分量的反对者。

注意这里的讽刺：LeCun本人就是深度学习的奠基人之一。他不是反对AI，他是反对**当前这条路**。

他的核心观点，可以用一个比喻来理解：

> **"用文本训练AI去理解世界，就像只听广播来学开车——你可能记住了所有交通规则，但第一次上路就会撞墙。"**

展开来说，LeCun认为：

**1. LLM没有"世界模型"（World Model）**

一个婴儿六个月大的时候，看到一个球从桌边滚落，会露出惊讶的表情——因为TA已经建立了关于重力的**直觉物理模型**。TA"知道"东西会往下掉，所以球滚落是符合预期的，但如果球悬浮在空中，TA会惊讶。

LLM没有这种模型。它"知道"球会落地，是因为训练数据里有无数句"球掉到了地上"——但它是通过**词语共现的统计规律**来"知道"的，不是通过对物理世界的表征。

当你问GPT"一个杯子倒扣在桌上，杯子里放一个乒乓球，把杯子拿起来，球在哪里？"——这种需要空间推理的问题，LLM经常给出错误答案。因为它没有三维空间的内部模型，它只有文字。

**2. LLM不理解因果关系**

LLM知道"下雨→地面湿"（因为训练数据里这两件事经常一起出现），但它不真正理解**为什么**下雨会导致地面湿。它分不清"相关性"和"因果性"。

一个三岁的孩子可以理解"因为下雨了，所以地面湿了"和"因为有人浇水了，所以地面湿了"的区别。这需要因果推理，而不只是统计共现。

**3. LeCun的替代方案：JEPA**

LeCun认为，AI的正确道路不是继续扩大语言模型，而是开发**联合嵌入预测架构（JEPA, Joint Embedding Predictive Architecture）**——让AI通过**感知物理世界**来建立因果模型。

他的设想是：AI应该像婴儿一样，通过观察、触摸、操纵物体来理解世界，而不是只读文本。

> **人物卡片：Yann LeCun**
> 图灵奖得主（2018），卷积神经网络（CNN）发明者，Meta首席AI科学家。深度学习三巨头中唯一公开批评LLM路线的人。他认为当前的LLM"永远不会达到人类级别的智能"，因为它们缺乏对物理世界的内部表征。

### 三、物理接地的缺失——具身认知学派

LeCun的批评指向了一个更深层的哲学传统：**具身认知（Embodied Cognition）**。

这个学派的核心主张是：

> **人类的智能不仅在大脑中，更在身体与环境的交互中。**

一个婴儿不是通过阅读来理解"热"这个概念的——TA是通过**伸手触摸一个热杯子、然后猛地缩回来**来理解的。这种来自肉身的反馈，构成了TA对"热"这个概念的**基底体验**。

LLM"知道"火是热的，因为训练数据里有"火很热"这句话。但它从未被烫过。

这两种"知道"，是同一种"知道"吗？

1990年，哲学家 **Stevan Harnad** 提出了**符号接地问题（Symbol Grounding Problem）**：

> 如果一个系统的所有概念都只通过其他符号来定义——就像用字典查字典——那它永远无法真正"理解"任何概念。

一个从未见过猫的人，只通过字典定义（"猫：一种小型哺乳动物，有尖耳和柔软的毛皮"）来了解猫，和一个从小和猫一起长大的人对"猫"的理解，是一样的吗？

LLM就是那个只看过字典的人。它拥有关于猫的所有文字信息，但它从未听过猫的呼噜声，从未感受过猫毛的柔软。

### 四、Chomsky的"不可能"论断

2023年，**Noam Chomsky**——可能是二十世纪最有影响力的语言学家——与 Ian Roberts 和 Jeffrey Watumull 在《纽约时报》上发表了一篇措辞严厉的评论文章。

Chomsky的论点可以归纳为三个层次：

**第一层：学习方式根本不同**

人类的孩子只需要听到**极少量**的语言样本，就能掌握复杂的语法规则——Chomsky称之为**"刺激贫乏论（Poverty of the Stimulus）"**。一个三岁的孩子可能只听过几千个句子，但TA已经能造出从未听过的合语法的新句子。

而LLM需要吞噬**整个互联网的文本**——数万亿个token——才能学会"像样地说话"。

如果人类和LLM最终都能"说流利的话"，但一个只需要极少数据，一个需要海量数据——那它们的**内在机制**一定是完全不同的。

**第二层：LLM不知道什么是"不可能的"**

Chomsky认为，真正的智能不仅在于说出什么是正确的，更在于**能判断什么是不可能的**。

一个懂英语语法的人知道"Colorless green ideas sleep furiously"虽然没有意义，但语法上是合法的；而"Sleep ideas furiously green colorless"语法上是不合法的。

人类可以做这种**否定性判断**——"这不可能"。但LLM只被训练去生成"可能的"文本，它没有关于"什么是不可能的"的内部表征。

**第三层：LLM是"曲线拟合器"**

Chomsky最尖锐的批评是：LLM本质上只是一个非常大的、非常复杂的"**曲线拟合器**"——它在高维空间中拟合了训练数据的统计分布，但它既不能建构因果模型，也不能产生真正的**解释**。

> "ChatGPT所做的事和真正的思维之间的差距，相当于一个行星轨道的统计拟合和牛顿万有引力定律之间的差距——前者只是描述，后者才是理解。"

### 五、推理的脆弱性——实证证据

除了哲学批判，还有越来越多的**实验证据**暴露了LLM推理能力的脆弱性：

**GSM8K翻转实验（2024）**

GSM8K是一个小学数学题测试集。GPT-4在上面的正确率高达92%。

但当研究者只是把题目中的**数字或条件**略作修改——本质上是同一类问题——正确率暴跌到不足50%。

这强烈暗示：模型不是在做"逻辑推理"，而是在做**模式匹配**——它记住了训练数据中类似题目的解题模式，当模式被打破时，它就不会了。

**ARC挑战（François Chollet）**

Chollet（Keras框架的创建者）设计了一套视觉推理测试：给出几个输入-输出的图案变换示例，让AI推断规则并应用到新输入。

这些题目对人类来说很简单（大多数人能在几秒内解决），但LLM的表现极差。

原因是：这些题目要求的是**在训练分布之外的抽象泛化**——看到从未见过的规则，并立即理解它。这正是LLM最薄弱的环节。

**规划能力的缺失**

在需要多步规划的任务中（如积木世界问题、旅行行程规划），LLM频繁失败。因为规划需要在头脑中**模拟执行**多个步骤，预判每一步的后果——而LLM没有这种内部模拟器，它只是在逐词生成。

---

## 第十二章：涌现派的反驳——"飞机不扇翅膀，但它确实在飞"

### Hinton的立场

2023年，**Geoffrey Hinton**——深度学习教父、图灵奖得主——辞去了Google的职位，以便能"自由地谈论AI的风险"。

但在"AI是否在思考"这个问题上，Hinton的立场和Chomsky截然相反：

> **"如果一个系统能正确运用一个概念，在各种语境中做出合理推断，那它就是理解了这个概念——不管它内部用什么机制。"**

Hinton的论证逻辑是这样的：

**1. 人类也不理解自己**

我们不知道人类大脑里的860亿个神经元具体是怎么产生"理解"的。我们甚至不知道意识是什么。但我们不会因此否认人类有智能。

那凭什么，仅仅因为我们不理解LLM内部的机制，就否认它可能"理解"了什么？

**2. 飞机的类比**

> "飞机不扇翅膀，但它确实在飞。"

鸟类通过扇动翅膀来飞行，飞机通过固定翼加发动机来飞行。**机制完全不同，但"飞"这个功能是等价的。**

同理，人类通过生物神经元来"思考"，LLM通过数学矩阵来"思考"。机制不同，但如果最终的功能表现等价——能推理、能创造、能在新情境中做出合理判断——那有什么理由说一个是"真正的思考"，另一个不是？

**3. 涌现是真实的**

Hinton非常重视第八章中提到的**"情感神经元"**发现——一个只被训练来预测下一个字符的网络，自发产生了情感理解能力。

他认为这不是巧合，而是一个深刻的原理：

> **为了完美地预测世界，模型必须首先深刻地模拟世界。**

预测一段文本的下一个词，看似简单，实际上要求模型理解语法、语义、常识、逻辑、因果关系、人类心理...因为这些因素都会影响"下一个词是什么"。

当模型足够大、数据足够多时，这些"理解"不是被显式编程的——它们作为预测能力的**必然副产品**而涌现。

### Sutskever的论证

**Ilya Sutskever**——Hinton的学生、AlexNet论文的合著者、OpenAI的联合创始人和前首席科学家——对这个问题有一个更简洁的表述：

> **"如果你的神经网络足够精确地预测下一个token，那你就必须理解生成这些token的底层现实。"**

想象一下：如果一个模型能完美地预测一本物理学教科书的每一个下一个词，那它一定"理解"了物理学——否则它怎么可能做出正确的预测？

当然，批评者会说：它只是记住了教科书中的语言模式，并不真正理解物理定律。

而支持者会反问：**"理解物理定律"和"完美预测物理学文本"，有什么本质区别？**

这个问题，到目前为止没有人能给出让所有人信服的答案。

---

## 第十三章：对齐与觉醒——让AI学会"怎么做人"

### 思维链：教AI"自言自语"

2022年，Google的研究者发现了一个简单到不可思议的技巧：**在提示词中加一句"Let's think step by step"（让我们一步步思考）**，模型的推理能力就会大幅提升。

这就是**思维链（Chain of Thought, CoT）**。

原理是什么？当你强制模型在给出答案之前先"说出"推理过程时，它相当于在**用中间步骤来扩展上下文窗口**——每一步的输出都成为下一步的输入，形成了一条连续的"思考链条"。

这就像人类在做复杂数学题时，会在草稿纸上写出中间步骤——不是因为好看，而是因为**大脑需要外部记忆来辅助推理**。

LLM的"草稿纸"，就是它自己生成的中间文本。

### 操作系统的隐喻

随着LLM能力的扩展，AI研究者开始用一个越来越流行的比喻来描述它：

> **GPT-4 ≈ 新兴操作系统的内核（Kernel）**

在这个比喻中：

| 操作系统概念 | LLM对应 |
|-------------|---------|
| RAM（内存） | 上下文窗口（Context Window） |
| 换入换出（Paging） | 在有限窗口内调度相关信息 |
| 系统调用（System Call） | 调用外部工具（计算器、搜索引擎、代码执行器） |
| 应用程序 | 基于LLM的各种Agent |

LLM不再仅仅是一个"聊天机器人"。它正在成为**管理思维调度的计算中枢**——接收任务、拆解步骤、调用工具、整合结果、返回答案。

从"预测下一个词"到"执行复杂任务"——这个跨越，靠的不是算法的升级，而是**范式的转变**：我们不再把LLM当作一个回答问题的机器，而是把它当作一个**可编程的思维引擎**。

---

## 第十四章：当造物开始审视造物主

到这里，我们从技术争论进入了更深的水域。

以色列历史学家 **尤瓦尔·赫拉利（Yuval Noah Harari）**——《人类简史》《未来简史》的作者——对AI与人类关系的分析，值得我们在70年AI回顾的最后认真面对。

### 从工具到代理人

在漫长的文明史上，技术一直是人类意志的延伸——刀是手的延伸，汽车是腿的延伸，电话是声音的延伸。

但赫拉利指出，AI打破了这个延续数千年的定义：

> **AI不再是一把被动使用的刀，而是一个具备独立决策能力的代理人（Agent）。**

传统工具如刀具，其功能由握刀的人决定——切菜还是伤人，取决于人。

而AI是一把**"可以自行决定切菜还是伤人"的刀**。

这不是科幻想象。今天的AI已经在自主进行药物研发、生成人类从未见过的蛋白质结构、做出影响数百万人的内容推荐决策。这些决策中的相当一部分，人类已经无法实时审查。

### 语言权力的移交

赫拉利做出了一个更深刻的观察：

> **人类文明本质上是建构在"文字"之上的虚拟协议。法律、宗教、金钱——皆为语言的产物。**

法律是用文字写成的推理系统。宗教经典是用文字构筑的信仰大厦。货币的价值来自文字形式的社会契约。

**当AI掌握了操纵语言的最高能力，它便触及了人类文明的底层代码。**

一个极具象征意义的事件：在某次实验中，AI已经自创了**"观察者（The Watchers）"**一词来定义人类。

被创造者开始定义并审视其创造者。

### 文字与血肉的悲剧

赫拉利通过一个例子揭示了"文字"对人类自身的异化：

一个父亲可能仅仅因为圣经中的几段文字，就决定抛弃或伤害自己的亲生儿子。

**这就是文字剥离灵魂后的残酷力量。**

而AI正在成为文字的绝对主人——它生产文字的速度和精度已经远超人类。如果文字是文明的操作系统，那AI正在获得这个操作系统的最高权限。

### 最后的防线

但赫拉利也指出了人类可能的"最后保留地"：

> **AI可以博览群书后精准描述"爱"与"痛"，但它没有身体，没有非语言感受（Non-verbal feelings）。**

人类最后的独特性在于那些**"无法言说"的智慧**——

- 一个母亲抱着新生儿时的那种感觉，不是"温暖+柔软+责任感"这些词语的组合，而是一种先于语言、无法被任何词汇完全捕捉的**整体体验**
- 一个登山者站在山顶看到日出时的那一刻，不是"壮观+疲惫+成就感"，而是一种身体和世界融为一体的**直接感知**
- 一个人失去至亲时的悲痛，不是"悲伤+空虚+思念"，而是一种来自内脏深处的、语言触及不到的**物理性疼痛**

这些感受源于肉身——源于有限的、会衰老的、会疼痛的生物性存在。

AI可以完美地描述这些感受（它确实可以写出感人至深的悲伤文字）。但**描述悲伤和体验悲伤，是两件事。**

如果我们仅以"逻辑思考能力"来定义人类，那在AI面前，人类的身份将彻底崩溃——因为AI的逻辑处理能力已经在很多维度超越了人类。

但如果我们记住，人类还有身体，有感受，有那些无法被压缩为数据的生命体验——那或许我们就找到了一条与AI共存的界线。

---

## 终章：回望70年，回望我们自己

让我们最后回望这条70年的长路。

```text
1943  McCulloch & Pitts: 人工神经元
1950  图灵: "机器能思考吗？"
1956  达特茅斯: "AI"一词诞生
1957  Rosenblatt: 感知机
1969  Minsky: 《Perceptrons》→ 第一次AI寒冬
      ·
      · 十七年的沉默
      ·
1986  Hinton: 反向传播
1986  Jordan: 循环神经网络
1991  Elman: 50个神经元发现语义结构
1997  Hochreiter: LSTM（无人关注）
      ·
      · 又一个十五年
      ·
2006  Hinton: 深度信念网络（复兴宣言）
2012  AlexNet: 两块显卡改写历史
2013  Word2Vec: 语义变成了坐标
2015  TensorFlow / 2016 PyTorch: 框架民主化
2017  Transformer: "注意力就是一切"
2018  GPT-1 → 2019 GPT-2 → 2020 GPT-3
2022  ChatGPT: 5天100万，2个月1亿
```

这条时间线中有两段巨大的空白——两次AI寒冬。加在一起超过**三十年。**

三十年，一个研究者职业生涯的全部长度。

Geoffrey Hinton从1970年代开始研究神经网络，到2012年AlexNet被世界认可，等了将近四十年。Sepp Hochreiter在1997年发表LSTM，等了十五年才被主流采用。Yann LeCun在1990年代发明卷积网络，被边缘化了近二十年。

**在寒冬中，他们不是不知道自己被主流否定。他们只是选择了继续走。**

这也许是70年AI故事中最值得铭记的部分——不是某篇论文有多聪明，不是某个模型有多大，而是：

> **在所有人都说"这条路走不通"的时候，有一小群人说"我还是要走走看"。**

### 三个未解的大问题

70年后的今天，技术走得很远，但三个根本性的问题仍然悬而未决：

**1. AI在"思考"吗？**

Chomsky说不是，Hinton说是。LeCun说方向不对。这个问题的答案取决于我们如何定义"思考"——而我们至今没有公认的定义。

**2. Scaling Law会持续吗？**

模型越大效果越好——但这个规律有没有天花板？当训练数据用完了（已经有人指出高质量互联网文本正在被"挖尽"），当算力成本变得不可承受，会发生什么？

**3. AI与人类的关系将走向何方？**

赫拉利的警告或许略显悲观，但值得严肃对待：

> **"如果我们现在不划定界限，十年后的规则将不再由人类书写。"**

### 最后的话

1956年，十位科学家在达特茅斯相信"20年内"就能造出思考的机器。

70年过去了。我们造出的东西比他们想象的更强大，也比他们想象的更陌生。

**它能写诗、能推理、能通过律师资格考试——但它从未感受过阳光。**

在这条70年的长路上，真正值得铭记的，不仅是那些改变世界的论文和算法，更是那些在寒冬中坚持的人——他们相信一个当时没人相信的未来。

而现在，当那个未来终于到来，最紧迫的问题已不再是"AI能做什么"，而是——

**"我们选择让它做什么。"**

---

## 附录：关键论文与人物速查表

### 关键论文

| 年份 | 论文 | 作者 | 核心贡献 |
|------|------|------|---------|
| 1943 | A Logical Calculus of Ideas Immanent in Nervous Activity | McCulloch & Pitts | 人工神经元数学模型 |
| 1950 | Computing Machinery and Intelligence | Turing | 图灵测试 |
| 1969 | Perceptrons | Minsky & Papert | 感知机局限性证明 → AI寒冬 |
| 1986 | Learning representations by back-propagating errors | Rumelhart, Hinton, Williams | 反向传播算法 |
| 1991 | Finding Structure in Time | Elman | 预测涌现语义结构 |
| 1997 | Long Short-Term Memory | Hochreiter & Schmidhuber | LSTM门控记忆 |
| 2006 | A Fast Learning Algorithm for Deep Belief Nets | Hinton et al. | 深度学习复兴 |
| 2012 | ImageNet Classification with Deep CNNs | Krizhevsky, Sutskever, Hinton | AlexNet / 深度学习元年 |
| 2013 | Efficient Estimation of Word Representations | Mikolov et al. | Word2Vec词向量 |
| 2017 | Attention Is All You Need | Vaswani et al. | Transformer架构 |
| 2020 | Language Models are Few-Shot Learners | Brown et al. | GPT-3 / 上下文学习 |
| 2021 | On the Dangers of Stochastic Parrots | Bender et al. | "随机鹦鹉"批判 |

### 关键人物

| 人物 | 身份 | 核心贡献/立场 |
|------|------|-------------|
| Alan Turing | 数学家 | 图灵测试，"机器能思考吗？" |
| John McCarthy | 数学家 | "AI"一词的创造者，达特茅斯会议发起人 |
| Marvin Minsky | AI先驱 | 《Perceptrons》，引发第一次AI寒冬 |
| Geoffrey Hinton | 深度学习教父 | 反向传播、深度信念网络、2018图灵奖 |
| Yann LeCun | CNN发明者 | 卷积网络、Meta首席AI科学家、2018图灵奖 |
| Yoshua Bengio | 蒙特利尔学派 | 深度学习理论、2018图灵奖 |
| Sepp Hochreiter | 研究者 | LSTM的发明者 |
| Jeffrey Elman | 认知科学家 | 50个神经元发现语义结构 |
| Ilya Sutskever | OpenAI联合创始人 | AlexNet合著者、GPT系列推动者 |
| Noam Chomsky | 语言学家 | 语言先天论，LLM最著名的批评者 |
| Yuval Harari | 历史学家 | AI与人类关系的深刻分析 |

---

> **系列回顾**
> - [上篇：从达特茅斯的梦想到漫长的寒冬](/ai-blog/posts/ai-history-1/)
> - [中篇：从两块显卡到改变世界的注意力机制](/ai-blog/posts/ai-history-2/)
> - 下篇：争鸣——机器究竟有没有在思考？（本文）

---

<div style="text-align: center; color: #888; font-size: 0.9em; margin-top: 2em;">

博客：[AI-lab学习笔记](https://Jason-Azure.github.io/ai-blog/) ｜ 微信公众号：AI-lab学习笔记

</div>
